{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation with Attention\n",
    "## English to Portuguese Translation\n",
    "\n",
    "This notebook demonstrates training and inference for a neural machine translation model using LSTM with attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup Python Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory (project root) to Python path\n",
    "sys.path.append('..')\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(\"✅ Path configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.translator import Translator\n",
    "from utils.data_loader import prepare_datasets, MAX_VOCAB_SIZE\n",
    "from utils.metrics import masked_loss, masked_acc\n",
    "import inference\n",
    "\n",
    "print(f\"✅ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"✅ NumPy version: {np.__version__}\")\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "UNITS = 256\n",
    "EPOCHS = 20\n",
    "STEPS_PER_EPOCH = 500\n",
    "\n",
    "print(f\"Vocabulary Size: {MAX_VOCAB_SIZE}\")\n",
    "print(f\"LSTM Units: {UNITS}\")\n",
    "print(f\"Training Epochs: {EPOCHS}\")\n",
    "print(f\"Steps per Epoch: {STEPS_PER_EPOCH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading data...\")\n",
    "train_data, val_data, english_vectorizer, portuguese_vectorizer = prepare_datasets()\n",
    "\n",
    "# Initialize inference module\n",
    "inference.initialize_vectorizers()\n",
    "\n",
    "print(f\"\\n✅ Data loaded successfully!\")\n",
    "print(f\"English vocabulary size: {english_vectorizer.vocabulary_size()}\")\n",
    "print(f\"Portuguese vocabulary size: {portuguese_vectorizer.vocabulary_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample vocabulary\n",
    "print(\"First 10 English words:\")\n",
    "print(english_vectorizer.get_vocabulary()[:10])\n",
    "\n",
    "print(\"\\nFirst 10 Portuguese words:\")\n",
    "print(portuguese_vectorizer.get_vocabulary()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a batch\n",
    "for (context, target_in), target_out in train_data.take(1):\n",
    "    print(f\"Batch size: {context.shape[0]}\")\n",
    "    print(f\"Context shape (English): {context.shape}\")\n",
    "    print(f\"Target input shape (Portuguese): {target_in.shape}\")\n",
    "    print(f\"Target output shape (Portuguese): {target_out.shape}\")\n",
    "    \n",
    "    print(f\"\\nFirst example:\")\n",
    "    print(f\"English tokens: {context[0].numpy()}\")\n",
    "    print(f\"Portuguese input tokens: {target_in[0].numpy()}\")\n",
    "    print(f\"Portuguese output tokens: {target_out[0].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "translator = Translator(MAX_VOCAB_SIZE, UNITS)\n",
    "\n",
    "# Compile\n",
    "translator.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=masked_loss,\n",
    "    metrics=[masked_acc, masked_loss]\n",
    ")\n",
    "\n",
    "print(\"✅ Model created and compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model\n",
    "\n",
    "**Note:** Training will take approximately 5-10 minutes per epoch depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = translator.fit(\n",
    "    train_data.repeat(),\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=50,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)]\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Model Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['masked_acc'], label='Training Accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_masked_acc'], label='Validation Accuracy', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Model Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"\\nFinal Training Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Training Accuracy: {history.history['masked_acc'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_masked_acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoints directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../checkpoints', exist_ok=True)\n",
    "\n",
    "# Save weights\n",
    "translator.save_weights('../checkpoints/translator_weights.h5')\n",
    "print(\"✅ Model weights saved to checkpoints/translator_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Translations\n",
    "\n",
    "### 8.1 Greedy Decoding (Temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"I love languages\",\n",
    "    \"How are you?\",\n",
    "    \"Good morning\",\n",
    "    \"Thank you very much\",\n",
    "    \"Where is the bathroom?\",\n",
    "    \"I am learning Portuguese\",\n",
    "    \"The weather is beautiful today\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GREEDY DECODING (Temperature = 0.0)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    translation, logit, _ = inference.translate(translator, sentence, temperature=0.0)\n",
    "    print(f\"\\nEN: {sentence}\")\n",
    "    print(f\"PT: {translation}\")\n",
    "    print(f\"Confidence: {logit:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Sampling with Different Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different temperatures\n",
    "test_sentence = \"I love languages\"\n",
    "temperatures = [0.0, 0.3, 0.6, 1.0]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"TEMPERATURE SAMPLING: '{test_sentence}'\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for temp in temperatures:\n",
    "    translation, logit, _ = inference.translate(translator, test_sentence, temperature=temp)\n",
    "    print(f\"\\nTemp {temp:.1f}: {translation}\")\n",
    "    print(f\"Logit: {logit:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Multiple Samples at Same Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple samples to see diversity\n",
    "test_sentence = \"I love languages\"\n",
    "num_samples = 5\n",
    "temperature = 0.6\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MULTIPLE SAMPLES (Temperature = {temperature})\")\n",
    "print(f\"Input: '{test_sentence}'\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    translation, logit, _ = inference.translate(translator, test_sentence, temperature=temperature)\n",
    "    print(f\"\\nSample {i+1}: {translation}\")\n",
    "    print(f\"Logit: {logit:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Minimum Bayes Risk (MBR) Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBR decoding\n",
    "test_sentence = \"I love languages\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MBR DECODING: '{test_sentence}'\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "translation, candidates = inference.mbr_decode(\n",
    "    translator, \n",
    "    test_sentence, \n",
    "    n_samples=10, \n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "print(\"\\nCandidate Translations:\")\n",
    "for i, candidate in enumerate(candidates, 1):\n",
    "    print(f\"{i:2d}. {candidate}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✅ SELECTED TRANSLATION: {translation}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Compare Greedy vs MBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different decoding strategies\n",
    "test_sentences_comparison = [\n",
    "    \"I love programming\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"Hello, how are you today?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: Greedy vs MBR Decoding\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for sentence in test_sentences_comparison:\n",
    "    print(f\"\\nInput: {sentence}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Greedy\n",
    "    greedy_translation, _, _ = inference.translate(translator, sentence, temperature=0.0)\n",
    "    print(f\"Greedy:  {greedy_translation}\")\n",
    "    \n",
    "    # MBR\n",
    "    mbr_translation, _ = inference.mbr_decode(translator, sentence, n_samples=8, temperature=0.6)\n",
    "    print(f\"MBR:     {mbr_translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Translation\n",
    "\n",
    "Change the sentence below and re-run the cell to translate your own sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive cell - change the sentence and run\n",
    "YOUR_SENTENCE = \"I love programming\"  # ← Change this!\n",
    "\n",
    "print(\"=\" * 70)\n",
    "translation, logit, _ = inference.translate(translator, YOUR_SENTENCE, temperature=0.0)\n",
    "print(f\"English:    {YOUR_SENTENCE}\")\n",
    "print(f\"Portuguese: {translation}\")\n",
    "print(f\"Confidence: {logit:.3f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Architecture Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model structure\n",
    "print(\"=\" * 70)\n",
    "print(\"ENCODER ARCHITECTURE\")\n",
    "print(\"=\" * 70)\n",
    "translator.encoder.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DECODER ARCHITECTURE\")\n",
    "print(\"=\" * 70)\n",
    "translator.decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Translation Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze translation quality by length\n",
    "short_sentences = [\n",
    "    \"Hello\",\n",
    "    \"Thank you\",\n",
    "    \"Good night\"\n",
    "]\n",
    "\n",
    "medium_sentences = [\n",
    "    \"I love learning new languages\",\n",
    "    \"The weather is nice today\",\n",
    "    \"Where can I find a restaurant?\"\n",
    "]\n",
    "\n",
    "long_sentences = [\n",
    "    \"I am very excited to learn Portuguese because it is a beautiful language\",\n",
    "    \"Machine translation has improved significantly with the advent of neural networks\",\n",
    "    \"Could you please tell me where I can find the nearest train station?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRANSLATION QUALITY BY SENTENCE LENGTH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for category, sentences in [(\"SHORT\", short_sentences), \n",
    "                             (\"MEDIUM\", medium_sentences), \n",
    "                             (\"LONG\", long_sentences)]:\n",
    "    print(f\"\\n{category} SENTENCES:\")\n",
    "    print(\"-\" * 70)\n",
    "    for sentence in sentences:\n",
    "        translation, logit, _ = inference.translate(translator, sentence, temperature=0.0)\n",
    "        print(f\"\\nEN: {sentence}\")\n",
    "        print(f\"PT: {translation}\")\n",
    "        print(f\"Confidence: {logit:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps\n",
    "\n",
    "### Model Performance\n",
    "- The model uses bidirectional LSTM encoder with attention mechanism\n",
    "- Training was performed with early stopping to prevent overfitting\n",
    "- Both greedy and MBR decoding strategies are available\n",
    "\n",
    "### Possible Improvements\n",
    "1. **Increase training data**: Use larger datasets for better generalization\n",
    "2. **Transformer architecture**: Replace LSTM with Transformer for better performance\n",
    "3. **Beam search**: Implement beam search decoding as alternative to greedy/MBR\n",
    "4. **Fine-tuning**: Train for more epochs or adjust learning rate schedule\n",
    "5. **Evaluation metrics**: Implement BLEU score for quantitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NMT-Attention)",
   "language": "python",
   "name": "nmt-attention"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}